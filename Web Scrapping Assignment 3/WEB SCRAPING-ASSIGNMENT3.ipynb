{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22f28b87",
   "metadata": {},
   "source": [
    "# Question 6. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "874ece4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Selenium in c:\\users\\subham\\anaconda3\\lib\\site-packages (4.17.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from Selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from Selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from Selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from Selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from Selenium) (4.9.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\subham\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\subham\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\subham\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from trio~=0.17->Selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->Selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->Selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\subham\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->Selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->Selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d2a5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c746bc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data scraped.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def scrape_best_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    laptops_data = []\n",
    "\n",
    "    try:\n",
    "        driver.implicitly_wait(5)\n",
    "\n",
    "        for laptop_card in driver.find_elements(By.CLASS_NAME, 'TopNumbeHeading'):\n",
    "            laptop_name = laptop_card.find_element(By.TAG_NAME, 'h3').text\n",
    "            laptop_specs = laptop_card.find_element(By.TAG_NAME, 'p').text\n",
    "\n",
    "            laptop_details = {\n",
    "                \"Name\": laptop_name,\n",
    "                \"Specifications\": laptop_specs\n",
    "            }\n",
    "\n",
    "            laptops_data.append(laptop_details)\n",
    "\n",
    "        return laptops_data\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gaming_laptops_data = scrape_best_gaming_laptops()\n",
    "\n",
    "    if gaming_laptops_data:\n",
    "        for laptop in gaming_laptops_data:\n",
    "            print(laptop)\n",
    "    else:\n",
    "        print(\"No data scraped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec81921",
   "metadata": {},
   "source": [
    "Question 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21003193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = (\"https://www.forbes.com/billionaires/\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        billionaires_data = []\n",
    "\n",
    "        for card in soup.find_all('div', class_='personCard'):\n",
    "            rank = card.find('div', class_='rank').text.strip()\n",
    "            name = card.find('div', class_='name').text.strip()\n",
    "            net_worth = card.find('div', class_='netWorth').text.strip()\n",
    "            age = card.find('div', class_='age').text.strip()\n",
    "            citizenship = card.find('div', class_='countryOfCitizenship').text.strip()\n",
    "            source = card.find('div', class_='source').text.strip()\n",
    "            industry = card.find('div', class_='category').text.strip()\n",
    "\n",
    "            billionaire_details = {\n",
    "                \"Rank\": rank,\n",
    "                \"Name\": name,\n",
    "                \"Net worth\": net_worth,\n",
    "                \"Age\": age,\n",
    "                \"Citizenship\": citizenship,\n",
    "                \"Source\": source,\n",
    "                \"Industry\": industry\n",
    "            }\n",
    "\n",
    "            billionaires_data.append(billionaire_details)\n",
    "\n",
    "        return billionaires_data\n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch data. Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billionaires_data = scrape_forbes_billionaires()\n",
    "\n",
    "    if billionaires_data:\n",
    "        for billionaire in billionaires_data:\n",
    "            print(billionaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029a97d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_elements_by_css_selector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     last_height \u001b[38;5;241m=\u001b[39m new_height\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Find all the comments on the page\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m comments \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements_by_css_selector(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mytd-comment-thread-renderer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Initialize an empty list to store the comments\u001b[39;00m\n\u001b[0;32m     32\u001b[0m comment_list \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_elements_by_css_selector'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the YouTube video page\n",
    "driver.get(\"https://www.youtube.com/watch?v=iwIeyWv2ABI\")\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Scroll to the bottom of the page to load all the comments\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll down the page by 1000 pixels\n",
    "    driver.execute_script(\"window.scrollTo(0, arguments[0]);\", last_height)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Calculate the new scroll height and compare it with the last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Find all the comments on the page\n",
    "comments = driver.find_elements_by_css_selector(\"ytd-comment-thread-renderer\")\n",
    "\n",
    "# Initialize an empty list to store the comments\n",
    "comment_list = []\n",
    "\n",
    "# Iterate through each comment and extract the required information\n",
    "for comment in comments:\n",
    "    try:\n",
    "        # Extract the comment text\n",
    "        comment_text = comment.find_element_by_css_selector(\"yt-formatted-string.style-scope.ytd-comment-renderer\").text\n",
    "        # Extract the comment upvote count\n",
    "        upvote_count = comment.find_element_by_css_selector(\"yt-icon.size-medium.style-scope.ytd-toggle-button-renderer\").get_attribute(\"aria-label\")\n",
    "        upvote_count = int(upvote_count.split(\" \")[0])\n",
    "        # Extract the comment time\n",
    "        comment_time = comment.find_element_by_css_selector(\"span.time.style-scope.ytd-comment-renderer\").text\n",
    "\n",
    "        # Append the extracted information to the list\n",
    "        comment_list.append({\n",
    "            \"comment_text\": comment_text,\n",
    "            \"upvote_count\": upvote_count,\n",
    "            \"comment_time\": comment_time\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Print the number of comments extracted\n",
    "print(f\"Number of comments extracted: {len(comment_list)}\")\n",
    "\n",
    "# Close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79aaec",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30e96aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\subham\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\subham\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\subham\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9add3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "search_query = \"oneplus-nord\"\n",
    "\n",
    "url = f\"https://www.flipkart.com/search?q={search_query}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36 Edge/16.16299\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "products = soup.find_all(\"div\", class_=\"_4ddWXP\")\n",
    "product_list = []\n",
    "\n",
    "for product in products:\n",
    "    brand_name = product.find(\"div\", class_=\"_2WkVRZ\").text.strip()\n",
    "    smartphone_name = product.find(\"div\", class_=\"B_NuCI\").text.strip()\n",
    "    colour = product.find(\"div\", class_=\"tVe96B\")\n",
    "    if colour:\n",
    "        colour = colour.text.strip()\n",
    "    else:\n",
    "        colour = \"-\"\n",
    "    ram = product.find(\"div\", class_=\"f1QpjSc\")\n",
    "    if ram:\n",
    "        ram = ram.text.strip()\n",
    "    else:\n",
    "        ram = \"-\"\n",
    "    storage = product.find(\"div\", class_=\"CXW8mj\")\n",
    "    if storage:\n",
    "        storage = storage.text.strip()\n",
    "    else:\n",
    "        storage = \"-\"\n",
    "    primary_camera = product.find(\"div\", class_=\"_30xnlZ\")\n",
    "    if primary_camera:\n",
    "        primary_camera = primary_camera.text.strip()\n",
    "    else:\n",
    "        primary_camera = \"-\"\n",
    "    secondary_camera = product.find(\"div\", class_=\"_1kMSr2\")\n",
    "    if secondary_camera:\n",
    "        secondary_camera = secondary_camera.text.strip()\n",
    "    else:\n",
    "        secondary_camera = \"-\"\n",
    "    display_size = product.find(\"div\", class_=\"dF4oaz\")\n",
    "    if display_size:\n",
    "        display_size = display_size.text.strip()\n",
    "    else:\n",
    "        display_size = \"-\"\n",
    "    battery_capacity = product.find(\"div\", class_=\"fQQj5E\")\n",
    "    if battery_capacity:\n",
    "        battery_capacity = battery_capacity.text.strip()\n",
    "    else:\n",
    "        battery_capacity = \"-\"\n",
    "    price = product.find(\"div\", class_=\"_30jeq3\")\n",
    "    if price:\n",
    "        price = price.text.strip()\n",
    "    else:\n",
    "        price = \"-\"\n",
    "    product_url = \"https://www.flipkart.com\" + product.find(\"a\", class_=\"IRpwTa\")[\"href\"]\n",
    "\n",
    "    product_list.append([brand_name, smartphone_name, colour, ram, storage, primary_camera, secondary_camera,\n",
    "                         display_size, battery_capacity, price, product_url])\n",
    "\n",
    "df = pd.DataFrame(product_list, columns=[\"Brand Name\", \"Smartphone Name\", \"Colour\", \"RAM\", \"Storage(ROM)\",\n",
    "                                         \"Primary Camera\", \"Secondary Camera\", \"Display Size\", \"Battery Capacity\",\n",
    "                                         \"Price\", \"Product URL\"])\n",
    "df.to_csv(f\"{search_query}_smartphones.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d2f69ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Smartphone Name</th>\n",
       "      <th>Colour</th>\n",
       "      <th>RAM</th>\n",
       "      <th>Storage(ROM)</th>\n",
       "      <th>Primary Camera</th>\n",
       "      <th>Secondary Camera</th>\n",
       "      <th>Display Size</th>\n",
       "      <th>Battery Capacity</th>\n",
       "      <th>Price</th>\n",
       "      <th>Product URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Brand Name, Smartphone Name, Colour, RAM, Storage(ROM), Primary Camera, Secondary Camera, Display Size, Battery Capacity, Price, Product URL]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4d475",
   "metadata": {},
   "source": [
    "Question 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a797fbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlesearch-python\n",
      "  Downloading googlesearch-python-1.2.3.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from googlesearch-python) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from googlesearch-python) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\subham\\anaconda3\\lib\\site-packages (from requests>=2.20->googlesearch-python) (2023.7.22)\n",
      "Building wheels for collected packages: googlesearch-python\n",
      "  Building wheel for googlesearch-python (setup.py): started\n",
      "  Building wheel for googlesearch-python (setup.py): finished with status 'done'\n",
      "  Created wheel for googlesearch-python: filename=googlesearch_python-1.2.3-py3-none-any.whl size=4224 sha256=219486ece8ec642e1561751b6c5f2dcc3574da4912c104dcb902e3c66227f649\n",
      "  Stored in directory: c:\\users\\subham\\appdata\\local\\pip\\cache\\wheels\\c3\\a9\\82\\de72a96d14538012479281dfda86c35b6da183692b94f45818\n",
      "Successfully built googlesearch-python\n",
      "Installing collected packages: googlesearch-python\n",
      "Successfully installed googlesearch-python-1.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googlesearch-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f66ace01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude: 47.6061389\n",
      "Longitude: -122.3328481\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "city = \"Seattle\"\n",
    "\n",
    "base_url = \"https://www.google.com/maps/place/{}/\"\n",
    "\n",
    "response = requests.get(base_url.format(city))\n",
    "\n",
    "match = re.search(r\"@(\\-?\\d+\\.\\d+),\\s*(\\-?\\d+\\.\\d+)\", response.text)\n",
    "\n",
    "\n",
    "if match:\n",
    "    lat, lng = match.groups()\n",
    "    print(\"Latitude: {}\\nLongitude: {}\".format(lat, lng))\n",
    "else:\n",
    "    print(\"Could not find latitude and longitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c15ac",
   "metadata": {},
   "source": [
    "Question 7: Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0775cfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=121.0.6167.161)\nStacktrace:\n\tGetHandleVerifier [0x00007FF7BF145E42+3538674]\n\t(No symbol) [0x00007FF7BED64C02]\n\t(No symbol) [0x00007FF7BEC15AEB]\n\t(No symbol) [0x00007FF7BEBF288C]\n\t(No symbol) [0x00007FF7BEC85DD7]\n\t(No symbol) [0x00007FF7BEC9B40F]\n\t(No symbol) [0x00007FF7BEC7EE53]\n\t(No symbol) [0x00007FF7BEC4F514]\n\t(No symbol) [0x00007FF7BEC50631]\n\tGetHandleVerifier [0x00007FF7BF176CAD+3738973]\n\tGetHandleVerifier [0x00007FF7BF1CC506+4089270]\n\tGetHandleVerifier [0x00007FF7BF1C4823+4057299]\n\tGetHandleVerifier [0x00007FF7BEE95C49+720121]\n\t(No symbol) [0x00007FF7BED7126F]\n\t(No symbol) [0x00007FF7BED6C304]\n\t(No symbol) [0x00007FF7BED6C432]\n\t(No symbol) [0x00007FF7BED5BD04]\n\tBaseThreadInitThunk [0x00007FFD0DE5257D+29]\n\tRtlUserThreadStart [0x00007FFD1014AA58+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.forbes.com/billionaires/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m wait \u001b[38;5;241m=\u001b[39m WebDriverWait(driver, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m table \u001b[38;5;241m=\u001b[39m wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable.datatable\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     14\u001b[0m rows \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39mfind_elements_by_css_selector(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr[role=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m rank \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:96\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m         value \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver)\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[0;32m     98\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py:84\u001b[0m, in \u001b[0;36mpresence_of_element_located.<locals>._predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver\u001b[38;5;241m.\u001b[39mfind_element(\u001b[38;5;241m*\u001b[39mlocator)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:741\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    738\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    739\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENT, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=121.0.6167.161)\nStacktrace:\n\tGetHandleVerifier [0x00007FF7BF145E42+3538674]\n\t(No symbol) [0x00007FF7BED64C02]\n\t(No symbol) [0x00007FF7BEC15AEB]\n\t(No symbol) [0x00007FF7BEBF288C]\n\t(No symbol) [0x00007FF7BEC85DD7]\n\t(No symbol) [0x00007FF7BEC9B40F]\n\t(No symbol) [0x00007FF7BEC7EE53]\n\t(No symbol) [0x00007FF7BEC4F514]\n\t(No symbol) [0x00007FF7BEC50631]\n\tGetHandleVerifier [0x00007FF7BF176CAD+3738973]\n\tGetHandleVerifier [0x00007FF7BF1CC506+4089270]\n\tGetHandleVerifier [0x00007FF7BF1C4823+4057299]\n\tGetHandleVerifier [0x00007FF7BEE95C49+720121]\n\t(No symbol) [0x00007FF7BED7126F]\n\t(No symbol) [0x00007FF7BED6C304]\n\t(No symbol) [0x00007FF7BED6C432]\n\t(No symbol) [0x00007FF7BED5BD04]\n\tBaseThreadInitThunk [0x00007FFD0DE5257D+29]\n\tRtlUserThreadStart [0x00007FFD1014AA58+40]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.forbes.com/billionaires/\")\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "table = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"table.datatable\")))\n",
    "rows = table.find_elements_by_css_selector(\"tr[role='row']\")\n",
    "\n",
    "rank = []\n",
    "name = []\n",
    "net_worth = []\n",
    "age = []\n",
    "citizenship = []\n",
    "source = []\n",
    "industry = []\n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_elements_by_css_selector(\"td\")\n",
    "    rank.append(cells[0].text)\n",
    "    name.append(cells[1].text)\n",
    "    net_worth.append(cells[2].text)\n",
    "    age.append(cells[3].text)\n",
    "    citizenship.append(cells[4].text)\n",
    "    source.append(cells[5].text)\n",
    "    industry.append(cells[6].text)\n",
    "\n",
    "df = pd.DataFrame({\"Rank\": rank, \"Name\": name, \"Net worth\": net_worth, \"Age\": age, \"Citizenship\": citizenship, \"Source\": source, \"Industry\": industry})\n",
    "\n",
    "print(df)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5daf172",
   "metadata": {},
   "source": [
    "Question 9: Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5beb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get(\"https://www.hostelworld.com/hostels/london\")\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "hostel_list = soup.find(\"div\", {\"class\": \"hostellist\"})\n",
    "\n",
    "name = []\n",
    "distance = []\n",
    "ratings = []\n",
    "total_reviews = []\n",
    "overall_reviews = []\n",
    "privates_from_price = []\n",
    "dorms_from_price = []\n",
    "facilities = []\n",
    "property_description = []\n",
    "\n",
    "hostel_cards = hostel_list.find_all(\"article\", {\"class\": \"hostelcard\"})\n",
    "\n",
    "for hostel in hostel_cards:\n",
    "    name_element = hostel.find(\"h2\", {\"class\": \"hostelname\"})\n",
    "    name.append(name_element.text.strip() if name_element else \"\")\n",
    "\n",
    "    distance_element = hostel.find(\"span\", {\"class\": \"distance\"})\n",
    "    distance.append(distance_element.text.strip() if distance_element else \"\")\n",
    "\n",
    "    ratings_element = hostel.find(\"span\", {\"class\": \"ratings\"})\n",
    "    ratings.append(ratings_element.text.strip() if ratings_element else \"\")\n",
    "\n",
    "    total_reviews_element = hostel.find(\"span\", {\"class\": \"totalreviews\"})\n",
    "    total_reviews.append(total_reviews_element.text.strip() if total_reviews_element else \"\")\n",
    "\n",
    "    overall_reviews_element = hostel.find(\"div\", {\"class\": \"reviewscore\"})\n",
    "    overall_reviews.append(overall_reviews_element.text.strip() if overall_reviews_element else \"\")\n",
    "\n",
    "    privates_from_price_element = hostel.find(\"span\", {\"class\": \"price pernight amount privates\"})\n",
    "    privates_from_price.append(privates_from_price_element.text.strip() if privates_from_price_element else \"\")\n",
    "\n",
    "    dorms_from_price_element = hostel.find(\"span\", {\"class\": \"price pernight amount dorms\"})\n",
    "    dorms_from_price.append(dorms_from_price_element.text.strip() if dorms_from_price_element else \"\")\n",
    "\n",
    "    facilities_element = hostel.find(\"div\", {\"class\": \"facilities\"})\n",
    "    facilities.append(facilities_element.text.strip() if facilities_element else \"\")\n",
    "\n",
    "    description_element = hostel.find(\"p\", {\"class\": \"description\"})\n",
    "    property_description.append(description_element.text.strip() if description_element else \"\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Name\": name, \"Distance\": distance, \"Ratings\": ratings, \"Total Reviews\": total_reviews,\n",
    "                  \"Overall Reviews\": overall_reviews, \"Privates From Price\": privates_from_price,\n",
    "                  \"Dorms From Price\": dorms_from_price, \"Facilities\": facilities,\n",
    "                  \"Property Description\": property_description})\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
